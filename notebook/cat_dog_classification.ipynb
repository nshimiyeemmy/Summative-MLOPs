{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cat and Dog Classification - ML Pipeline\n",
        "\n",
        "This notebook demonstrates the complete machine learning pipeline for classifying images of cats and dogs.\n",
        "\n",
        "## Objectives:\n",
        "1. Data Acquisition and Preprocessing\n",
        "2. Model Creation using Transfer Learning\n",
        "3. Model Training with Optimization Techniques\n",
        "4. Model Evaluation with Multiple Metrics\n",
        "5. Model Testing and Prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.20.0\n",
            "GPU Available: []\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, optimizers, callbacks\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "import sys\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append('../src')\n",
        "from preprocessing import prepare_dataset, augment_data\n",
        "from model import create_model, train_model, evaluate_model, save_model_metadata\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Acquisition and Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset...\n",
            "Loading training images...\n",
            "Loading test images...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Load dataset (using subset for faster training - remove max_train/max_test for full dataset)\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m X_train, X_test, y_train, y_test = \u001b[43mprepare_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_cats_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dogs_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_cats_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dogs_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_train\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use 1000 images per class for training (remove for full dataset)\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_test\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# Use 200 images per class for testing (remove for full dataset)\u001b[39;49;00m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining set shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest set shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_test.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LENOVO\\Music\\Summative-MLOPs\\notebook\\../src\\preprocessing.py:72\u001b[39m, in \u001b[36mprepare_dataset\u001b[39m\u001b[34m(train_cats_dir, train_dogs_dir, test_cats_dir, test_dogs_dir, target_size, max_train, max_test)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading test images...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m test_cats = load_images_from_directory(test_cats_dir, target_size, max_test)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m test_dogs = \u001b[43mload_images_from_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dogs_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Combine and create labels\u001b[39;00m\n\u001b[32m     75\u001b[39m X_train = np.concatenate([train_cats, train_dogs], axis=\u001b[32m0\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LENOVO\\Music\\Summative-MLOPs\\notebook\\../src\\preprocessing.py:35\u001b[39m, in \u001b[36mload_images_from_directory\u001b[39m\u001b[34m(directory, target_size, max_images)\u001b[39m\n\u001b[32m     32\u001b[39m img_path = os.path.join(directory, img_file)\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Load and resize image\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     img = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m     img = img.convert(\u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     37\u001b[39m     img = img.resize(target_size)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LENOVO\\miniconda3\\Lib\\site-packages\\PIL\\Image.py:3513\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[32m   3512\u001b[39m     filename = os.fspath(fp)\n\u001b[32m-> \u001b[39m\u001b[32m3513\u001b[39m     fp = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3514\u001b[39m     exclusive_fp = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   3515\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Define data paths\n",
        "train_cats_dir = '../data/train/cats'\n",
        "train_dogs_dir = '../data/train/dogs'\n",
        "test_cats_dir = '../data/test/cats'\n",
        "test_dogs_dir = '../data/test/dogs'\n",
        "\n",
        "# Load dataset (using subset for faster training - remove max_train/max_test for full dataset)\n",
        "print(\"Loading dataset...\")\n",
        "X_train, X_test, y_train, y_test = prepare_dataset(\n",
        "    train_cats_dir, train_dogs_dir,\n",
        "    test_cats_dir, test_dogs_dir,\n",
        "    target_size=(224, 224),\n",
        "    max_train=1000,  # Use 1000 images per class for training (remove for full dataset)\n",
        "    max_test=200     # Use 200 images per class for testing (remove for full dataset)\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n",
        "print(f\"Training labels shape: {y_train.shape}\")\n",
        "print(f\"Test labels shape: {y_test.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize sample images\n",
        "fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
        "fig.suptitle('Sample Training Images', fontsize=16)\n",
        "\n",
        "for i in range(4):\n",
        "    # Cat images\n",
        "    cat_idx = np.where(y_train == 0)[0][i]\n",
        "    axes[0, i].imshow(X_train[cat_idx])\n",
        "    axes[0, i].set_title('Cat')\n",
        "    axes[0, i].axis('off')\n",
        "    \n",
        "    # Dog images\n",
        "    dog_idx = np.where(y_train == 1)[0][i]\n",
        "    axes[1, i].imshow(X_train[dog_idx])\n",
        "    axes[1, i].set_title('Dog')\n",
        "    axes[1, i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data augmentation\n",
        "print(\"Augmenting training data...\")\n",
        "X_train_aug, y_train_aug = augment_data(X_train, y_train, augment_factor=1)\n",
        "print(f\"Augmented training set shape: {X_train_aug.shape}\")\n",
        "\n",
        "# Split training data into train and validation sets\n",
        "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
        "    X_train_aug, y_train_aug, \n",
        "    test_size=0.2, \n",
        "    random_state=42,\n",
        "    stratify=y_train_aug\n",
        ")\n",
        "\n",
        "print(f\"\\nFinal training set: {X_train_final.shape}\")\n",
        "print(f\"Validation set: {X_val.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Model Creation with Transfer Learning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model using transfer learning with MobileNetV2\n",
        "print(\"Creating model with MobileNetV2 (pretrained on ImageNet)...\")\n",
        "model = create_model(\n",
        "    input_shape=(224, 224, 3),\n",
        "    num_classes=2,\n",
        "    use_pretrained=True  # Using pretrained weights for transfer learning\n",
        ")\n",
        "\n",
        "# Display model architecture\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Training with Optimization Techniques\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 32\n",
        "MODEL_PATH = '../models/cat_dog_model.h5'\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting model training...\")\n",
        "print(f\"Epochs: {EPOCHS}, Batch Size: {BATCH_SIZE}\")\n",
        "\n",
        "history = train_model(\n",
        "    model, X_train_final, y_train_final, X_val, y_val,\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    model_save_path=MODEL_PATH\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Accuracy plot\n",
        "axes[0].plot(history.history['accuracy'], label='Training Accuracy')\n",
        "axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "axes[0].set_title('Model Accuracy')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True)\n",
        "\n",
        "# Loss plot\n",
        "axes[1].plot(history.history['loss'], label='Training Loss')\n",
        "axes[1].plot(history.history['val_loss'], label='Validation Loss')\n",
        "axes[1].set_title('Model Loss')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Evaluation with Multiple Metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the best model (saved during training)\n",
        "from model import load_model\n",
        "best_model = load_model(MODEL_PATH)\n",
        "\n",
        "# Evaluate model on test set\n",
        "print(\"Evaluating model on test set...\")\n",
        "metrics = evaluate_model(best_model, X_test, y_test)\n",
        "\n",
        "# Display metrics\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL EVALUATION METRICS\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Accuracy:  {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)\")\n",
        "print(f\"Precision: {metrics['precision']:.4f} ({metrics['precision']*100:.2f}%)\")\n",
        "print(f\"Recall:    {metrics['recall']:.4f} ({metrics['recall']*100:.2f}%)\")\n",
        "print(f\"F1 Score:  {metrics['f1_score']:.4f} ({metrics['f1_score']*100:.2f}%)\")\n",
        "print(f\"Test Loss: {metrics['test_loss']:.4f}\")\n",
        "print(f\"Test Accuracy: {metrics['test_accuracy']:.4f} ({metrics['test_accuracy']*100:.2f}%)\")\n",
        "print(\"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "cm = np.array(metrics['confusion_matrix'])\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Cat', 'Dog'], \n",
        "            yticklabels=['Cat', 'Dog'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "# Calculate per-class metrics\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(f\"\\nPer-class metrics:\")\n",
        "print(f\"True Negatives (Cat correctly identified): {tn}\")\n",
        "print(f\"False Positives (Cat misclassified as Dog): {fp}\")\n",
        "print(f\"False Negatives (Dog misclassified as Cat): {fn}\")\n",
        "print(f\"True Positives (Dog correctly identified): {tp}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Classification Report\n",
        "y_pred = np.argmax(best_model.predict(X_test, verbose=0), axis=1)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Cat', 'Dog']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize predictions on test set\n",
        "fig, axes = plt.subplots(3, 6, figsize=(18, 9))\n",
        "fig.suptitle('Sample Test Predictions', fontsize=16)\n",
        "\n",
        "predictions = best_model.predict(X_test[:18], verbose=0)\n",
        "predicted_classes = np.argmax(predictions, axis=1)\n",
        "confidences = np.max(predictions, axis=1)\n",
        "\n",
        "class_names = ['Cat', 'Dog']\n",
        "\n",
        "for i in range(18):\n",
        "    row = i // 6\n",
        "    col = i % 6\n",
        "    \n",
        "    axes[row, col].imshow(X_test[i])\n",
        "    true_label = class_names[int(y_test[i])]\n",
        "    pred_label = class_names[predicted_classes[i]]\n",
        "    confidence = confidences[i]\n",
        "    \n",
        "    color = 'green' if true_label == pred_label else 'red'\n",
        "    axes[row, col].set_title(f\"True: {true_label}\\nPred: {pred_label} ({confidence:.2f})\", \n",
        "                            color=color, fontsize=9)\n",
        "    axes[row, col].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Save Model Metadata\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model metadata\n",
        "save_model_metadata(metrics, '../models/model_metadata.json')\n",
        "print(\"Model metadata saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. ✅ Data preprocessing with augmentation\n",
        "2. ✅ Model creation using transfer learning (MobileNetV2)\n",
        "3. ✅ Training with optimization techniques (Early Stopping, Learning Rate Reduction, Model Checkpointing)\n",
        "4. ✅ Comprehensive evaluation with multiple metrics (Accuracy, Precision, Recall, F1 Score, Loss, Confusion Matrix)\n",
        "5. ✅ Model testing and visualization of predictions\n",
        "\n",
        "The model is now ready for deployment!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
